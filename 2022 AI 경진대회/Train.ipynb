{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DETR.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5CWL+Oo5VSueJcC3FLpB5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## DETR fine tune\n","대회명: 2022 AI 경진대회 <br>\n","기간: 2022/06/07 ~ 2022/06/21 <br>\n","\n","짧다면 짧고 길다면 긴 기간동안 그동안 배웠던 내용들을 바탕으로 실질적인 프로젝트를 진행해보고 싶어서 경진대회에 참여하게 됐습니다. <br>\n","마지막 날 제출을 위해 test image(zip)를 드라이브에 업로드하려고 했는데 10시간 이상 예상 소요시간이 걸려 결국 마감때문에 제출 fail.... <br>\n","그래도 첫 AI 프로젝트이므로 포스트를 통해 공부하는 기회로 삼아보려고 합니다. <br>\n"],"metadata":{"id":"S67Ar8OUU_y7"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zBDOyxG4RmZy","executionInfo":{"status":"ok","timestamp":1655771561812,"user_tz":-540,"elapsed":23431,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"a512cdc5-c437-468e-86f2-2dee6d1cddfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/2022 AI 경진대회\n"]}],"source":["# ref: cs231n/assignments\n","from google.colab import drive\n","import sys\n","drive.mount('/content/drive')\n","FOLDERNAME = '2022 AI 경진대회/'\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"]},{"cell_type":"markdown","source":["## Environment\n","아무래도 경진대회이다 보니 주최측에서 특정 환경 하에서 코드를 일괄적으로 실행하기 위한 환경 설정 제한이 있습니다 이를 위한 setting 입니다."],"metadata":{"id":"f59mwX8r8pCw"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WofIQC2FJzn1","executionInfo":{"status":"ok","timestamp":1655477631513,"user_tz":-540,"elapsed":473,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"297cb19c-6ed1-4c99-980e-cf5e748b5de4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Jun 17 14:53:51 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# check GPU\n","!nvcc --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSQaDCSh9LwY","executionInfo":{"status":"ok","timestamp":1655684436256,"user_tz":-540,"elapsed":315,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"1dad9e9d-b903-4f90-9311-7806aee7fffc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Mon_Oct_12_20:09:46_PDT_2020\n","Cuda compilation tools, release 11.1, V11.1.105\n","Build cuda_11.1.TC455_06.29190527_0\n"]}]},{"cell_type":"code","source":["# check python version\n","import sys\n","print(sys.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKBp0cdq-p-S","executionInfo":{"status":"ok","timestamp":1655628595027,"user_tz":-540,"elapsed":292,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"8f38d32b-aa1b-44ca-93fc-d34003620bb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.7.13 (default, Apr 24 2022, 01:04:09) \n","[GCC 7.5.0]\n"]}]},{"cell_type":"code","source":["!pip uninstall torch torchvision\n","!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"],"metadata":{"id":"u5Iwj1vN9Wi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check pytorch version\n","import torch\n","\n","print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZnvwzJvAyOO","executionInfo":{"status":"ok","timestamp":1655628847650,"user_tz":-540,"elapsed":956,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"7db31a23-d949-45fb-8e24-1a6ee92ec100"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.8.0+cu111\n"]}]},{"cell_type":"markdown","source":["## DETR github clone"],"metadata":{"id":"qZu0iSKfM9j6"}},{"cell_type":"code","source":["# !git clone https://github.com/facebookresearch/detr.git "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-S_DTUQgRrBH","executionInfo":{"status":"ok","timestamp":1654759034877,"user_tz":-540,"elapsed":2435,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"c79d9baf-49d0-4771-e246-08589102addc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'detr'...\n","remote: Enumerating objects: 260, done.\u001b[K\n","remote: Total 260 (delta 0), reused 0 (delta 0), pack-reused 260\u001b[K\n","Receiving objects: 100% (260/260), 12.85 MiB | 14.55 MiB/s, done.\n","Resolving deltas: 100% (142/142), done.\n"]}]},{"cell_type":"markdown","source":["## Unzip data\n","directory 구조<br>\n","\n","2022 AI 경진대회 <br>\n","\n","├── data <br>\n","│   ├── Train_imgs <br>\n","│   ├── Test_imgs <br>\n","│   ├── Train_label.json <br>\n","│   └── Test_images_info.json(Empty Json) <br>\n","│<br>\n","├── detr(github clone) <br>\n","│<br>\n","├── checkpiont(model 저장소) <br>\n","│<br>\n","├── work_dirs <br>\n","│   └── log.txt <br>\n","│   └── tune.txt <br>\n","│   └── best_tune.txt <br>\n","│ <br>\n","├── Train.ipynb <br>\n","│    <br>\n","├── Inference.ipynb <br>\n","\n","\n","ref: 2022 AI 경진대회 baseline"],"metadata":{"id":"ZdabCoej_63I"}},{"cell_type":"code","source":["# train unzip\n","\n","# import os\n","# train_root = FOLDERNAME + '/data/Train_imgs'\n","# %cd /content/drive/My\\ Drive/$train_root\n","# !unzip -qq \"/content/drive/My Drive/2022 AI 경진대회/data/train\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjfNkY3upZdx","executionInfo":{"status":"ok","timestamp":1654828239327,"user_tz":-540,"elapsed":99032,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"160de815-569a-4110-b66b-7e98c8e71db2"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/imgDetection/2022 AI 경진대회/data\n"]}]},{"cell_type":"code","source":["# test unzip\n","\n","# import os\n","# test_root = FOLDERNAME + '/data/Test_imgs'\n","# %cd /content/drive/My\\ Drive/$test_root\n","# !unzip -qq \"/content/drive/My Drive/2022 AI 경진대회/data/test.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6y4UMPKt237","executionInfo":{"status":"ok","timestamp":1655757334223,"user_tz":-540,"elapsed":427,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"23467044-1682-4362-e385-21eaaf386dcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/2022 AI 경진대회/data/Test_imgs\n","[/content/drive/My Drive/2022 AI 경진대회/data/test.zip]\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of /content/drive/My Drive/2022 AI 경진대회/data/test.zip or\n","        /content/drive/My Drive/2022 AI 경진대회/data/test.zip.zip, and cannot find /content/drive/My Drive/2022 AI 경진대회/data/test.zip.ZIP, period.\n"]}]},{"cell_type":"markdown","source":["## import"],"metadata":{"id":"6c-tg26bAn00"}},{"cell_type":"code","source":["import os\n","import math\n","from pathlib import Path\n","import numpy as np \n","import pandas as pd \n","import random\n","import json\n","import cv2\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import copy\n","\n","# Torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","\n","################# DETR FUCNTIONS FOR LOSS######################## \n","sys.path.append('./detr/')\n","\n","from detr.models.matcher import HungarianMatcher\n","from detr.models.detr import SetCriterion\n","from detr.datasets import coco\n","from detr.models.detr import PostProcess\n","from detr.datasets.coco_eval import CocoEvaluator\n","from detr.datasets import get_coco_api_from_dataset\n","import detr.util.misc as utils  "],"metadata":{"id":"VDCApUAtU1zn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## config, data, model"],"metadata":{"id":"-I9PzAf3A86-"}},{"cell_type":"markdown","source":["## 1. config\n","```python\n","def max_objects(root):\n","  return maximum number of objects in train image dataset\n","```\n","DETR의 경우 num_queries(bounding box 갯수)를 통해 매번 back ground와 object를 가려내는 알고리즘으로 이미지 내에 객체의 갯수가 num_queries보다 많을 경우 모든 객체를 검출해내지 못합니다. 따라서 image의 객체 갯수를 train image를 통해 대략 알아내 보는 알고리즘을 작성해봤습니다. <br>\n","물론 실질적인 문제 해결(ex. 자율주행에의 적용)에서는 적합한 방법이 아니지만 경진대회 같은 형식에서는 결국 주어진 상황 속에서 최고의 결과물을 뽑아내는 것이 중요하니... <br>\n","확인결과 대략 30개의 object가 최대 갯수로 산출이 되었었습니다. detr 설계자들에 따르면 이미지 내에 최대 object 갯수가 default num queries(100)를 넘지 않는 경우 thin object를 효과적으로 검출하면서 성능을 좋게 유지하는 num queries는 default(100)라고 말합니다. <br>\n","ref: https://github.com/facebookresearch/detr/issues/126"],"metadata":{"id":"W7zktGl6g6ae"}},{"cell_type":"code","source":["# to know maximum number of objects in overall train image set\n","def max_objects(root):\n","  '''\n","  inputs:\n","    - root(str): root folder of train label file\n","  returns:\n","    - max_num(int): maximum number of objects in overall train image set\n","  '''\n","  with open(os.path.join(root, 'Train_label.json')) as j:\n","    file = json.load(j)\n","    anns = file['annotations']\n","  max_num = 0\n","  obj = 0\n","  prev = anns[0]['image_id']\n","  for ann in anns:\n","    now = ann['image_id']\n","    if prev != now:\n","      obj = 0\n","    else:\n","      obj += 1\n","      if obj > max_num:\n","        max_num = obj\n","\n","    prev = now\n","  return max_num"],"metadata":{"id":"_kFVKbg-J40G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = {\n","    \"LR\": 0.0009967324530542163,\n","    \"null_class_coef\": 0.9,\n","    \"init_embed\": True,\n","    \"freeze\": True,\n","    \"batch_size\": 2,\n","    \"max_norm\": 0.18121613279447868, \n","    \"optim\": \"AdamW\",\n","    \"start_epoch\": 4,\n","    \"end_epoch\": 4\n","    }\n","\n","num_classes = 14 + 1 # 14: true num classes, 1: no_object(background)\n","\n","# checkpoint\n","# checkpoint = os.path.join(os.getcwd(), 'checkpoint')\n","\n","# data root\n","root = Path('/content/drive/My Drive/' + FOLDERNAME + '/data')\n","num_queries = max_objects(root) * 2 # test 이미지 dataset의 최대 object 갯수가 train dataset 최대 object 갯수보다 많을 수 있으니\n","\n","'''\n","code taken from github repo detr , 'code present in engine.py'\n","'''\n","\n","matcher = HungarianMatcher()\n","\n","weight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n","\n","losses = ['labels', 'boxes', 'cardinality']\n","\n","# postprocessors\n","postprocessors = {'bbox': PostProcess()} "],"metadata":{"id":"dhFdRvQuV6kz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset\n","원래 coco dataset의 directory는 다음과 같습니다. <br>\n","path/to/coco/ <br>\n","&nbsp;  annotations/  # annotation json files <br>\n","&nbsp;  train2017/    # train images <br>\n","&nbsp;  val2017/      # val images <br>\n","\n","주최측에서 하나의 폴더 안에 모든 train image와 이에 대한 하나의 annotation 파일을 제공해주었습니다. <br>\n","이를 train image에서 일정부분 추출하여 이에 matching되는 annotation 파일을 만들어 분리를 해야하지만 적합한 방법을 찾지 못했습니다. <br>\n","그래서 모든 train image를 불러와 각각 train transform, val transform을 통해 두 dataset를 만들고 rand permutation을 통해 split을 해주었습니다. <br>\n","ref: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"],"metadata":{"id":"uEocaNCOBF3r"}},{"cell_type":"code","source":["def get_dataset(root, split_ratio = 0.8, mini = False):\n","  '''\n","  Custom Dataset \n","    For validation data set, I just use same file and annotation, but different transformation\n","    and then split these by random permutation indices (code from pytorch object detection example)\n","  \n","  inputs:\n","    - root(str): data root folder\n","    - split_ratio(float): ratio of spliting data into train & val\n","    - mini(bool): whether to shirink dataset or not (usually for overfitting to check model's performance fast)\n","  returns:\n","    - dataset_train(dataset): dataset after train trasformation\n","    - dataset_val(dataset): dataset after val transformation\n","  '''\n","  PATHS = {\n","      'train': (root / 'Train_imgs', root / 'Train_label.json'),\n","      'val': (root/'Train_imgs', root/ 'Train_label.json')\n","  }\n","  \n","  img_folder, ann_file = PATHS['train']\n","  dataset_train = coco.CocoDetection(img_folder, ann_file, transforms=coco.make_coco_transforms('train'), return_masks=False)\n","  dataset_val = coco.CocoDetection(img_folder, ann_file, transforms=coco.make_coco_transforms('val'), return_masks=False)\n","  \n","  # split dataset into train and val\n","  dset_size = len(dataset_train)\n","  indices = torch.randperm(len(dataset_train)).tolist()\n","  dataset_train = torch.utils.data.Subset(dataset_train, indices[:int(dset_size * split_ratio)])\n","  dataset_val = torch.utils.data.Subset(dataset_val, indices[int(dset_size * split_ratio):]) \n","\n","  if mini:\n","    dataset_train = torch.utils.data.Subset(dataset_train, torch.arange(120))\n","    dataset_val = torch.utils.data.Subset(dataset_val, torch.arange(40))\n","\n","  return dataset_train, dataset_val"],"metadata":{"id":"euYRWEdA9wVQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model(DETR)\n","Fine Tuning을 하기 위해 <br>\n","1) pretrained model load <br>\n","2) init class embed <br>\n","3) init query_embed if init_embed <br>\n","4) gradient = False if freeze (except for class_embed or query_embed) <br>\n","(cf: class_embed: class 예측하는 layer (nn.Linear) query_embed: num_queries에 따라 bounding box 예측 layer)\n"],"metadata":{"id":"hnWTKs9SlZqW"}},{"cell_type":"code","source":["class DETRModel(nn.Module):\n","    def __init__(self, num_classes, num_queries, init_embed=False):\n","      '''\n","      for finetuning change the fc(num classes) and num queries\n","      make pretrained DETR model and modify number of classes and queires of it.\n","      inputs:\n","        - num_classes(int)\n","        - num_queries(int)\n","      '''\n","      super(DETRModel,self).__init__()\n","      self.num_classes = num_classes\n","      self.num_queries = num_queries\n","      \n","      self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n","      self.in_features = self.model.class_embed.in_features\n","\n","\n","      self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n","      torch.nn.init.normal_(self.model.class_embed.weight, mean=0.0, std=0.02)\n","      torch.nn.init.zeros_(self.model.class_embed.bias)\n","      if init_embed:\n","        self.hidden_dim = self.model.query_embed.embedding_dim\n","        self.model.query_embed = nn.Embedding(self.num_queries, self.hidden_dim)\n","      \n","    def forward(self,images):\n","      '''\n","      inputs: \n","        - images(list):\n","      returns:\n","        - output: img, dict\n","      '''\n","      return self.model(images)\n","\n","def get_model(num_classes=14+1, num_queries=100, freeze = True, load = False, init_embed = False):\n","  '''\n","  inputs:\n","    - freeze(bool): not use backprop of the network except for the linear classification(class_embed)\n","    - load(bool): whether to load from checkpoint or not\n","  returns:\n","    - model\n","  '''\n","  model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n","\n","  if freeze:\n","    for name, param in model.named_parameters():\n","      if init_embed:\n","        if 'query_embed' in name or 'class_embed' in name:\n","          param.requires_grad = True\n","        else:\n","          param.requires_grad = False\n","      else:\n","        if 'class_embed' in name:\n","          param.requires_grad = True\n","        else:\n","          param.requires_grad = False\n","\n","  if load:\n","    PATH = os.path.join(os.getcwd(), 'detr_best_.pth')\n","    model.load_state_dict(torch.load(PATH))\n","\n","  return model"],"metadata":{"id":"mHUodL10aVrM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## util functions\n","대부분의 경우 detr/engin.py copy & paste입니다."],"metadata":{"id":"o_7fIOmJCU1Y"}},{"cell_type":"code","source":["def train_fn(data_loader, model, criterion, optimizer, device, epoch, max_norm):\n","  model.train()\n","  criterion.train()\n","  ###\n","  metric_logger = utils.MetricLogger(delimiter=\"  \")\n","  metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n","  metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n","  header = 'Epoch: [{}]'.format(epoch)\n","  print_freq = 10\n","\n","  for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n","      \n","      images = list(image.to(device) for image in images)\n","      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","      \n","\n","      output = model(images)\n","      \n","      loss_dict = criterion(output, targets)\n","      weight_dict = criterion.weight_dict\n","      \n","      losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n","      ###\n","      loss_dict_reduced = utils.reduce_dict(loss_dict)\n","      loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n","                                    for k, v in loss_dict_reduced.items()}\n","      loss_dict_reduced_scaled = {k: v * weight_dict[k]\n","                                  for k, v in loss_dict_reduced.items() if k in weight_dict}\n","      losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n","\n","      loss_value = losses_reduced_scaled.item()\n","      \n","      optimizer.zero_grad()\n","\n","      losses.backward()\n","      if max_norm > 0:\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n","      optimizer.step()\n","      \n","      metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n","      metric_logger.update(class_error=loss_dict_reduced['class_error'])\n","      metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n","  # gather the stats from all processes\n","  metric_logger.synchronize_between_processes()\n","  print(\"Averaged stats:\", metric_logger)\n","  return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"],"metadata":{"id":"THep9NC0aiYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_fn(data_loader, model,criterion, postprocessors, device, base_ds):\n","    model.eval()\n","    criterion.eval()\n","    \n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n","    header = 'Test:'\n","    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n","    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n","    coco_evaluator = None # for fast, if you want to check IoU and other coco evaluations apply this line as comment\n","    \n","    with torch.no_grad():\n","        for images, targets in metric_logger.log_every(data_loader, 10, header):\n","            images = list(image.to(device) for image in images)\n","            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","            outputs = model(images)\n","        \n","            loss_dict = criterion(outputs, targets)\n","            weight_dict = criterion.weight_dict\n","            loss_dict_reduced = utils.reduce_dict(loss_dict)\n","            loss_dict_reduced_scaled = {k: v * weight_dict[k]\n","                                        for k, v in loss_dict_reduced.items() if k in weight_dict}\n","            loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n","                                          for k, v in loss_dict_reduced.items()}\n","            metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n","                                **loss_dict_reduced_scaled,\n","                                **loss_dict_reduced_unscaled)\n","            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n","\n","            orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n","            results = postprocessors['bbox'](outputs, orig_target_sizes)\n","\n","            res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n","            if coco_evaluator is not None:\n","                coco_evaluator.update(res)\n","\n","            metric_logger.synchronize_between_processes()\n","            print(\"Averaged stats:\", metric_logger)\n","\n","            if coco_evaluator is not None:\n","                coco_evaluator.synchronize_between_processes()\n","                coco_evaluator.eval_imgs['bbox'] = [coco_evaluator.eval_imgs['bbox']]\n","\n","\n","            if coco_evaluator is not None:\n","              coco_evaluator.accumulate()\n","              coco_evaluator.summarize()    \n","\n","            stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n","            \n","            if coco_evaluator is not None:\n","                if 'bbox' in postprocessors.keys():\n","                    stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n","\n","    return stats, coco_evaluator"],"metadata":{"id":"WwN35Gm6bYWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    return tuple(zip(*batch))"],"metadata":{"id":"WAE7VpY6bbhI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1) load & save <br>\n","colab 환경에서는 자주 런타임이 끊기기 때문에 checkpoint를 통해 계속해서 model을 추적해 나가야 했습니다. detr/engine.py에서 start epoch과 end eopch을 통해 이러한 설계방안을 채택한 것을 그대로 이용했습니다. <br>\n","2) dataset <br>\n","하나의 image dataset에서 split을 하는 것이라 random permutation을 매번 진행할 경우 두 dataset이 섞이는 경우가 생깁니다. cross validation을 적용할 것이기 아니기 때문에 seed를 통해 이러한 경우를 방지하려고 했습니다. <br>\n"],"metadata":{"id":"gtPrqV_JnO7a"}},{"cell_type":"code","source":["def run(config):\n","    torch.manual_seed(42) # for finetune, dataset \n","    model = get_model(num_classes = num_classes,\n","                 num_queries= num_queries,\n","                 freeze = config['freeze'],\n","                 load = True,\n","                 init_embed = config['init_embed'])\n","\n","    batch_size = config['batch_size']\n","    # dataset\n","    dataset_train, dataset_val = get_dataset(root, mini=False)\n","\n","    # make data loader\n","    sample_train = torch.utils.data.RandomSampler(dataset_train)\n","\n","    batch_sampler_train = torch.utils.data.BatchSampler(sample_train, \n","                                                    batch_size, drop_last=True)\n","    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n","\n","    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n","                                  collate_fn=collate_fn, num_workers=0)\n","    data_loader_val = DataLoader(dataset_val, batch_size, sampler=sampler_val,\n","                                 drop_last=False, collate_fn = collate_fn, num_workers=0)\n","    \n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    if config['optim'] == 'momentum':\n","      optimizer = torch.optim.SGD(model.parameters(), lr=config['LR'], momentum = 0.9)\n","    elif config['optim'] == 'AdamW':\n","      optimizer = torch.optim.AdamW(model.parameters(), lr=config['LR'])\n","\n","    if os.path.isfile(os.path.join(os.getcwd(), f\"checkpoint/detr{config['start_epoch']-1:04}.pth\")):\n","      ckpt_path = os.path.join(os.getcwd(), f\"checkpoint/detr{config['start_epoch']-1:04}.pth\")\n","      print('model, optimizer loading from %s' % (ckpt_path))\n","      ckpt = torch.load(ckpt_path)\n","      model.load_state_dict(ckpt['model_state_dict'])\n","      optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n","      for state in optimizer.state.values(): # https://developers-shack.tistory.com/6\n","       for k, v in state.items():\n","         if torch.is_tensor(v):\n","             state[k] = v.to(device)\n","    \n","    model = model.to(device)\n","    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = config['null_class_coef'], losses=losses)\n","    criterion = criterion.to(device)\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100)\n","\n","    if os.path.isfile(os.path.join(os.getcwd(), 'checkpoint/detr_best.pth')):\n","      checkpoint = torch.load(os.path.join(os.getcwd(), 'checkpoint/detr_best.pth'))\n","      best_loss = checkpoint['loss']\n","    else:\n","      best_loss = math.inf\n","\n","    print('#'*100)\n","    print(\"best_loss: \", best_loss)\n","    print('#'*100)\n","\n","    for epoch in range(config['start_epoch'], config['end_epoch'] + 1):\n","        train_stats = train_fn(data_loader_train, model,criterion, optimizer,device, epoch, config['max_norm'])\n","        lr_scheduler.step()\n","        test_stats, coco_evaluator= eval_fn(data_loader_val, model,criterion, postprocessors, device,base_ds = get_coco_api_from_dataset(dataset_val))\n","      \n","        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n","                     **{f'test_{k}': v for k, v in test_stats.items()},\n","                     'epoch': epoch,\n","                     }\n","\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': test_stats['loss']\n","        }, os.path.join(os.getcwd(), f'checkpoint/detr{epoch:04}.pth'))\n","\n","        if best_loss > test_stats['loss']:\n","          best_loss = test_stats['loss']\n","          torch.save({\n","            'epoch': copy.deepcopy(epoch),\n","            'model_state_dict': copy.deepcopy(model.state_dict()),\n","            'optimizer_state_dict': copy.deepcopy(optimizer.state_dict()),\n","            'loss': copy.deepcopy(test_stats['loss'])\n","        }, os.path.join(os.getcwd(), f'checkpoint/detr_best.pth'))\n","\n","        print('#'*100)\n","        print(\"best_loss: \", best_loss)\n","        print('#'*100)\n","\n","        with open(os.path.join(os.getcwd(), \"work_dirs/log.txt\"), \"a\") as f:\n","            f.write(json.dumps(log_stats) + \"\\n\")\n","\n","    # return model, test_stats # for hyperparameter tuning"],"metadata":{"id":"ojuNlHc5bgXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## hyperparameter tuning\n","모델의 성능을 최적으로 끌어올리기 위해 hyper parameter들을 random으로 조합하여 model의 결과값을 확인하였습니다. mini dataset을 통해 overfitting하려고 시도했으며 예상만큼 loss가 줄지는 않았습니다. 이에 대한 해석으로 <br>\n","1) model 설계 문제 <br>\n","2) DETR model의 특성(높은 성능을 위해 많은 epoch 필요) <br>\n","(cf: 논문 훈련 GPU : 16개의 V100 GPU <br>\n","epoch : 300 (Faster R-CNN의 경우 500) <br>\n"," 시간 : 72 시간 <br>\n","ref: https://keyog.tistory.com/32 ) <br>\n","2가지가 존재하는 것 같습니다. <br>\n","결과값들은 log, tune.txt에 모두 기록하여 분석이 가능하도록 설계하였습니다."],"metadata":{"id":"5wD53PqmCaAl"}},{"cell_type":"code","source":["# # hyper parameter tuning\n","\n","'''\n","hyperparameter search space(tune)\n","\n","search_space = {\n","    \"LR\": [10 ** random.uniform(-6, -1) for _ in range(50)],\n","    'null_class_coef': [0.5, 0.3, 0.7, 0.9],\n","    'init_embed': [True, False],\n","    'freeze': [True, False],\n","    'batch_size':[1, 2],\n","    'max_norm': [random.uniform(0.05, 0.2) for _ in range(5)],\n","    'optim': ['momentum', 'AdamW'],\n","    'start_epoch': 0,\n","    'end_epoch': 40\n","}\n","'''\n","\n","# num_sample = 10\n","# best_loss = math.inf\n","# best_cfg = None\n","# best_model = None\n","# for _ in range(num_sample):\n","#   sample = {}    \n","#   for k, v in search_space.items():\n","#     sample[k] = random.choice(v)\n","#   DETR, stats = run(sample)\n","#   if best_loss > stats['loss']:\n","#     best_loss = stats['loss']\n","#     best_cfg = sample\n","#     best_model = DETR\n","#   with open(os.path.join(os.getcwd(), \"work_dirs/tune.txt\"), \"a\") as f:\n","#     f.write(json.dumps(sample) + \"\\n\")\n","#     f.write(json.dumps(stats) + '\\n') \n","#     f.write('#'*500 + '\\n')\n","    \n","# with open(os.path.join(os.getcwd(), 'work_dirs/best_tune.txt'), \"a\") as f:\n","#   f.write(json.dumps(best_cfg) + '\\n')"],"metadata":{"id":"zQO80DvRvzYC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## train & eval\n","전체 train dataset 크기: 24,650 (1920 x 1080) <br>\n","train split: 24,650 x 0.8 <br>\n","val split: 24,650 x 0.2 <br>\n","1 epoch (train & val): 대략 4~5 시간 (colab pro)"],"metadata":{"id":"my4MBemrCg6r"}},{"cell_type":"code","source":["run(config)"],"metadata":{"id":"3dBHclLt-V6p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## visualize sample\n","모델의 학습 결과를 대략적으로 확인하기 위해 val dataset을 통해 bounding box와 label을 비교할 수 있도록 작성하였습니다. <br>\n","먼저, <br>\n","1) load from checkpoint(best model or latest model) <br>\n","2) get dataset(val) <br>\n","3) bounding box 처리 (x, y, c_x, c_y) -> (x1, y1, x2, y2) <br>\n","4) label 처리 (idx -> class name) <br>\n","5) pred output, ground truth output visualize\n"],"metadata":{"id":"QWp7L1SiIBcY"}},{"cell_type":"code","source":["# load best model, optimizer\n","\n","best_model = get_model()\n","if config['optim'] == 'momentum':\n","  optimizer = torch.optim.SGD(best_model.parameters(), lr=config['LR'], momentum = 0.9)\n","elif config['optim'] == 'AdamW':\n","  optimizer = torch.optim.AdamW(best_model.parameters(), lr=config['LR'])\n","\n","# checkpoint\n","ckpt_path = os.path.join(os.getcwd(), 'checkpoint/detr0003.pth')\n","ckpt = torch.load(ckpt_path)\n","best_model.load_state_dict(ckpt['model_state_dict'])\n","optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n","epoch = ckpt['epoch']\n","loss = ckpt['loss']"],"metadata":{"id":"8iiZoO2oCvcZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655772141303,"user_tz":-540,"elapsed":7622,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"8afdd1b7-dae7-4180-f9b2-83ebfeeee304"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_detr_main\n"]}]},{"cell_type":"code","source":["# label list\n","# 0: 'NO_OBJECT'\n","# 1: '세단(승용차)'\n","# 2: 'SUV'\n","# 3: '승합차\n","# 4: '버스'\n","# 5: '학원차량(통학버스)'\n","# 6: '트럭'\n","# 7: '택시'\n","# 8: '성인'\n","# 9: '어린이'\n","# 10: '오토바이'\n","# 11: '전동킥보드'\n","# 12: '자전거'\n","# 13: '유모차'\n","# 14: '쇼핑카트\n","\n","batch_size = 2\n","\n","# transform for inference\n","import torchvision.transforms as T\n","transform = T.Compose([\n","    T.Resize(800),\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","_, dataset_val = get_dataset(root, mini=True)\n","sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","data_loader_val = DataLoader(dataset_val, batch_size, sampler=sampler_val,\n","                              drop_last=False, collate_fn = collate_fn, num_workers=0)\n","imgs, tgts= {}, {}\n","for i, (x, y) in enumerate(data_loader_val):\n","  imgs[i], tgts[i] = x, y"],"metadata":{"id":"aDRmQrUsZR8P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655771930048,"user_tz":-540,"elapsed":60872,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"bdb1433f-9536-4632-86b5-4f3fa9657c3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.89s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=1.49s)\n","creating index...\n","index created!\n"]}]},{"cell_type":"code","source":["threshold = 0.3 # before submitting you should naively check by visualizing\n","torch.manual_seed(42)\n","\n","images = list(img.to(device) for img in imgs[10])\n","targets = [{k: v.to(device) for k, v in t.items()} for t in tgts[10]] \n","image_id = targets[0]['image_id']\n","with open(os.path.join(os.getcwd(), 'data/Train_label.json'), 'r') as file:\n","  json_file = json.load(file)\n","  images_info = json_file['images']\n","\n","file_name = images_info[image_id]['file_name']\n","img_path = os.path.join(os.getcwd(), 'data/Train_imgs/' + file_name)\n","img = Image.open(img_path).convert(\"RGB\")\n","W, H = img.size\n","with torch.no_grad():\n","  best_model.eval()\n","  best_model.to(device)\n","  im = transform(img)\n","  im = im.unsqueeze(0)\n","  outputs = best_model(im.to(device))\n","  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n","  keep = probas.max(-1).values > threshold\n","\n","  # for cv2.putText use Enligsh not Korean\n","  classes = ['NO_OBJECT', 'sedan', 'SUV', 'van',\n","            'bus', 'school bus', 'truck', 'taxi',\n","            'adult', 'child', 'motorcycle', 'kickboard',\n","            'bicycle', 'stroller', 'shopping cart']\n","  id_to_c = {i: cls for i, cls in enumerate(classes)}\n","\n","  labels = torch.argmax(outputs['pred_logits'][0, keep], dim=-1)\n","  pred_labels = [id_to_c[label] for label in labels.tolist()]\n","  gt_labels = [id_to_c[label.cpu().tolist()] for label in targets[0]['labels']]\n","  score, _ = torch.max(probas[keep], dim = -1)\n","\n","  x_c, y_c, w, h = outputs['pred_boxes'][0, keep].unbind(1)\n","  b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n","          (x_c + 0.5 * w), (y_c + 0.5 * h)]\n","  boxes = torch.stack(b, dim=1)\n","  preds = boxes * torch.tensor([W, H, W, H], dtype=torch.float32, device = device)\n","  gt_boxes = targets[0]['boxes'] * torch.tensor([W, H, W, H], dtype = torch.float32, device = device)\n","  gt_boxes_np= gt_boxes.detach().cpu().numpy()\n","  c_x, c_y, w, h = gt_boxes_np[:, 0], gt_boxes_np[:, 1], gt_boxes_np[:, 2], gt_boxes_np[:, 3]\n","  x1, y1, x2, y2 = (c_x - w/2).astype(np.int64), (c_y - h/2).astype(np.int64), (c_x + w/2).astype(np.int64), (c_y + h/2).astype(np.int64)\n","\n","  pred = np.array(preds.cpu(), dtype=int)\n","  plt.figure(figsize = (100,50))\n","  img_copy = img.copy()\n","  img_np = np.array(img_copy)\n","\n","  # pred box \n","  for i, p in enumerate(pred):\n","    cv2.rectangle(img_np,\n","                      (p[0], p[1]),\n","                      (p[2], p[3]),\n","                  color=(255, 0, 0), thickness = 5 )\n","    cv2.putText(img_np,\n","                pred_labels[i],\n","                (p[0], p[1]-10),\n","                cv2.FONT_HERSHEY_SIMPLEX,\n","                0.9,\n","                (255,0,0),\n","                2)\n","  # ground truth box\n","  for i in range(len(x1)):\n","    cv2.rectangle(img_np,\n","                      (x1[i].astype(int), y1[i].astype(int)),\n","                      (x2[i].astype(int), y2[i].astype(int)), color=(0, 255, 0), thickness = 10 )\n","    cv2.putText(img_np,\n","              gt_labels[i], \n","              ((x1[i] + x2[i])//2, (y1[i]+y2[i])//2),\n","              cv2.FONT_HERSHEY_SIMPLEX,\n","              0.9,\n","              (0,255,0),\n","              2)\n","    \n","  plt.imshow(img_np)\n","  plt.show()"],"metadata":{"id":"qqTV68toZc9Q","colab":{"base_uri":"https://localhost:8080/","height":792,"output_embedded_package_id":"1cP7u7rMNWofa6MriTpLWI16aOfwxp3mC"},"executionInfo":{"status":"ok","timestamp":1655772387512,"user_tz":-540,"elapsed":28197,"user":{"displayName":"임재혁","userId":"12415892552982450882"}},"outputId":"2e51d88b-f1ae-487a-b7e1-7defe4f74a95"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Inference and submit\n","1) Test_images_info.json(image info, coco format)을 통해 test image dataset에 접근 <br>\n","2) model에 transform(image) feed <br>\n","3) 결과값 dictionery 형태로 list에 append <br>\n","4) write json<br>\n","\n","cf) Test_images_info.json <br>\n","```python\n","{\n","  \"images\": [\n","    {\n","      \"file_name\": \"image1.png\",\n","      \"license\": null,\n","      \"coco_url\": null,\n","      \"height\": 1080,\n","      \"width\": 1920,\n","      \"data_captured\": null,\n","      \"flickr_url\": null,\n","      \"id\": 0\n","    },\n","    {\n","      \"file_name\": \"image2.png\",\n","      \"license\": null,\n","      \"coco_url\": null,\n","      \"height\": 1080,\n","      \"width\": 1920,\n","      \"data_captured\": null,\n","      \"flickr_url\": null,\n","      \"id\": 1\n","    },\n","    ...\n","  ]\n","}\n","```"],"metadata":{"id":"8xUGR_v-r_nf"}},{"cell_type":"code","source":["threshold = 0.3\n","\n","with open(os.path.join(os.getcwd(), 'data/Test_images_info.json'), 'r') as j:\n","  image_info = json.load(j)\n","\n","submission_anno = list()\n","for img_info in image_info['images']:\n","  file_name = img_info['file_name']\n","  img_path = os.path.join(os.getcwd(), 'data/test/images/' + file_name)\n","  img = Image.open(img_path).convert(\"RGB\")\n","  W, H = img.size\n","\n","  with torch.no_grad():\n","    model_copy.eval()\n","    im = transform(img)\n","    outputs = model_copy(im.unsqueeze(0))\n","    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n","    keep = probas.max(-1).values > threshold\n","    pred_labels = torch.argmax(outputs['pred_logits'][0, keep], dim=-1).tolist()\n","    scores, _ = torch.max(probas[keep], dim = -1)\n","    \n","    if len(pred_labels) == 0:\n","      continue\n","    \n","    x_c, y_c, w, h = outputs['pred_boxes'][0, keep].unbind(1)\n","    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n","         (w), (h)]\n","    boxes = torch.stack(b, dim=1)\n","    preds_b = boxes * torch.tensor([W, H, W, H], dtype=torch.float32)\n","    for i in range(len(pred_labels)):\n","      tmp_dict = dict()\n","      tmp_dict['image_id'] = img_info['id']\n","      tmp_dict['bbox'] = preds_b[i].tolist()\n","      tmp_dict['category_id'] = pred_labels[i]\n","      tmp_dict['score'] = scores[i].item()\n","      tmp_dict['segmentation'] = []\n","\n","      submission_anno.append(tmp_dict)"],"metadata":{"id":"iy4csr8bzeRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('./sample_submission.json','w',encoding='utf-8') as f:\n","    json.dump(submission_anno,f,ensure_ascii=False)"],"metadata":{"id":"HRBHB9FezoLx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 아쉬운점\n","개인 <br>\n","\n","1. 너무 준비되지 않은 상태에서 project에 혼자 도전하다보니 공부를 하는 것이 아닌 code copy & paste 형식으로 진행이 된 점<br>\n","2. directory split을 해결하지 못한 점 <br>\n","3. 제출도 해보지 못한 점 <br>\n","4. 성능이 너무 좋지 않게 나온 점 <br>\n","5. 많은 GPU 연산이 필요한 transformer(DETR) 모델이 아닌 다른 모델을 도전해 봤으면 어떤 결과가 나왔을 지에 대한 궁금점 <br>\n","\n","주최측 <br>\n","1. zip 파일로 data를 제공해주다보니 load하는 데에 시간을 너무 많이 소요 <br>\n","2. 고해상도의 image파일들을 너무 많이 제공해주다 보니 model 훈련에 시간, 자원이 너무 많이 소요됨 <br>\n","(학생이나 소규모 사업가 입장에서 부담이 되었을 것) <br>\n","3. train dataset을 통해 알 수 있듯이 annotaion이 적합하게 되어있지 않은 파일들이 존재\n"],"metadata":{"id":"1zf0B85gsCP0"}},{"cell_type":"markdown","source":["## 앞으로 학습 방향\n","이번 프로젝트를 통해 많은 detection open source, github들을 살펴보게 되었다. <br>\n","이를 통해 알게 된 점은 크게 <br>\n","1) argparse 라이브러리를 통한 parse <br>\n","2) metric logger를 통한 출력 <br>\n","3) coco dataset library를 통한 data 변환 <br>\n","이 주로 사용된다는 점이다. <br>\n","다양한 task(classification, detection, segmentation, NLP, ...)에 대한 알고리즘을 공부하면서 딥러닝 전반에 대한 공부가 필요한 시점인 것 같다."],"metadata":{"id":"K1MX3uH4tf7x"}},{"cell_type":"markdown","source":["code ref<br>\n","1) https://github.com/facebookresearch/detr.git <br>\n","2) https://www.kaggle.com/code/tanulsingh077/end-to-end-object-detection-with-transformers-detr/notebook <br>\n","3)  https://keyog.tistory.com/32 <br>\n","4) https://developers-shack.tistory.com/6 <br>\n","5)  https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"],"metadata":{"id":"hXAoqorI2LVv"}},{"cell_type":"markdown","source":["cf) 데이터의 경우 주최측 제한사항에도 있고 github에도 안 올라가니 모두 삭제"],"metadata":{"id":"6jaeMgdR1cxz"}}]}